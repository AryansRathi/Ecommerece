This assignment covers the Harris corner detector, RANSAC and the HOG descriptor for panorama stitching.
In this assignment, we will detect and match keypoints from multiple images to build a single panoramic image. This will involve several tasks:
In this section, you are going to implement Harris corner detector for keypoint localization. Review the lecture slides on Harris corner detector to understand how it works. The Harris detection algorithm can be divide into the following steps:
Step 1 is already done for you in the function <strong><code>harris_corners</code></strong> in <code>panorama.py</code>. Complete the function implementation and run the code below.
<em>-Hint: You may use the function <code>scipy.ndimage.filters.convolve</code>, which is already imported in <code>panoramy.py</code></em>
Once you implement the Harris detector correctly, you will be able to see small bright blobs around the corners of the sudoku grids and letters in the output corner response image. The function <code>corner_peaks</code> from <code>skimage.feature</code> performs non-maximum suppression to take local maxima of the response map and localize keypoints.
We are now able to localize keypoints in two images by running the Harris corner detector independently on them. Next question is, how do we determine which pair of keypoints come from corresponding locations in those two images? In order to <em>match</em> the detected keypoints, we must come up with a way to <em>describe</em> the keypoints based on their local appearance. Generally, each region around detected keypoint locations is converted into  a fixed-size vectors called <em>descriptors</em>.
In this section, you are going to implement the <strong><code>simple_descriptor</code></strong> function, where each keypoint is described by the normalized intensity of a small patch around it.
Next, implement the <strong><code>match_descriptors</code></strong> function to find good matches in two sets of descriptors. First, calculate Euclidean distance between all pairs of descriptors from image 1 and image 2. Then use this to determine if there is a good match: if the distance to the closest vector is significantly (by a given factor) smaller than the distance to the second-closest, we call it a match. The output of the function is an array where each row holds the indices of one pair of matching descriptors.
We now have a list of matched keypoints across the two images. We will use this to find a transformation matrix that maps points in the second image to the corresponding coordinates in the first image. In other words, if the point $p_1 = [y_1,x_1]$ in image 1 matches with $p_2=[y_2, x_2]$ in image 2, we need to find an affine transformation matrix $H$ such that
where $\tilde{p_1}$ and $\tilde{p_2}$ are homogenous coordinates of $p_1$ and $p_2$.
Note that it may be impossible to find the transformation $H$ that maps every point in image 2 exactly to the corresponding point in image 1. However, we can estimate the transformation matrix with least squares. Given $N$ matched keypoint pairs, let $X_1$ and $X_2$ be $N \times 3$ matrices whose rows are homogenous coordinates of corresponding keypoints in image 1 and image 2 respectively. Then, we can estimate $H$ by solving the least squares problem,
Implement <strong><code>fit_affine_matrix</code></strong> in <code>panorama.py</code>
<em>-Hint: read the <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html">documentation</a> about np.linalg.lstsq</em>
Next, the two warped images are merged to get a panorama. Your panorama may not look good at this point, but we will later use other techniques to get a better result.
Rather than directly feeding all our keypoint matches into <code>fit_affine_matrix</code> function, we can instead use RANSAC ("RANdom SAmple Consensus") to select only "inliers" to use for computing the transformation matrix.
The steps of RANSAC are:
Implement <strong><code>ransac</code></strong> in <code>panorama.py</code>, run through the following code to get a panorama. You can see the difference from the result we get without RANSAC.
We can now use the tranformation matrix $H$ computed using the robust matches to warp our images and create a better-looking panorama.
Implement <strong><code>hog_descriptor</code></strong> in <code>panorama.py</code> and run through the following code to get a panorama image.
Once we've described our keypoints with the HOG descriptor and have found matches between these keypoints, we can use RANSAC to select robust matches for computing the tranasformtion matrix.
Now we use the computed transformation matrix $H$ to warp our images and produce our panorama.
You will notice the blurry region and unpleasant lines in the middle of the final panoramic image. Using a very simple technique called linear blending, we can smooth out a lot of these artifacts from the panorama.
Currently, all the pixels in the overlapping region are weighted equally. However, since the pixels at the left and right ends of the overlap are very well complemented by the pixels in the other image, they can be made to contribute less to the final panorama.
Linear blending can be done with the following steps:
In <strong><code>linear_blend</code></strong> in <code>panorama.py</code> implement the linear blending scheme to make the panorama look more natural. This extra credit can be worth up to 1% of your final grade.
Implement <strong><code>stitch_multiple_images</code></strong> in <code>panorama.py</code> to stitch together an ordered chain of images. This extra credit can be worth up to 1 bonus point of your final grade.
Given a sequence of $m$ images ($I_1, I_2,...,I_m$), take every neighboring pair of images and compute the transformation matrix which converts points from the coordinate frame of $I_{i+1}$ to the frame of $I_{i}$. Then, select a reference image $I_{ref}$, which is in the middle of the chain. We want our final panorama image to be in the coordinate frame of $I_{ref}$.
<em>-Hint:</em>
